[{"id":0,"href":"/docs/abd/","title":"ABD Algorithm","section":"Docs","content":" Write # Get phase: vt-set = read vt pairs from majority of storage nodes select unique t such that t \u0026gt; max(t in vt-set)\nSet phase: write_request(v, t) on storage nodes storage nodes store vt only if t \u0026gt; their stored t storage nodes send ACK when majority ACKs are received return OK\nRead # Get phase vt-set = read vt pairs from majority of storage nodes select vt such that t = max(t in vt-set)\nSet phase write_request(v, t) on storage nodes storage nodes store vt only if t \u0026gt; their stored t storage nodes send ACK when majority ACKs are received return v\n"},{"id":1,"href":"/docs/model-checker/","title":"Model Checker in Go","section":"Docs","content":" Temporal logic # More than 2000 years ago, Greek philocersor Aristotle, as the first man ever systematically studied logic, has already raised concers about how time (futures in particular) plays a complex role in logical reasoning in his book On Interpretation. Since then, very little was developed for millennia. It was until 1947, a formalization of temporal functions was created by Polish mathematician Jerzy Los. Ever since 1950s, lots of theoretical works has been developed in this area. Eventually, temporal logic has find its biggest application in computer science as model checker.\nSome of them, like Computation Tree Logic (CTL) finds its application in computer science as model checkers.\nCTL models are defined as transition system $M = (S, \\to, L)$ where $S$ is the total set of states, $\\to \\subseteq S \\times S$ is the finite set of state transitions, and $L$ is the labeling function for states. Under the definition of $M$, a model starts from initial state $s_0 \\in S$, and generates all possible states by iteratively applying the well defined state transitions. Such model is the minimal abstraction needed to describe any algorithm, and a model checker can verify any temporal properties on state while exploring the state space, as a result checking the correctness of the algorithm.\nImaging a simple model contains two possible actions, $a_0$ and $a_1$ on any state. Ideally, the two actions will generate a full binary tree of states as shown in Figure 1, thus the name computation tree. But in general, the state graph might be\nan action may have no effect, i.e. the next state is equal to previous state.\n%%{init: {'theme':'dark'}}%% flowchart LR; s0 --a0--\u003e s1 s0 --a1--\u003e s2 s1 --a0--\u003e s3 s1 --a1--\u003e s4 s2 --a0--\u003e s5 s2 --a1--\u003e s6 Model checker is a type of program\nProbabilistic CTL\n"},{"id":2,"href":"/docs/topology-mapping/","title":"Topology Mapping Algorithm","section":"Docs","content":" ICS 2011\nFormalization of the Problem # Network is modeled by a weighted graph $H=(V_H, \\omega_H, R_H)$ where $V_H$ is set of vertices/nodes $\\omega_H(u, v) \\in \\mathbb{R}$ are the weighted edges with $u, v \\in V_H$ $R_H(u, v)$ is the routing function as a probability distribution of on the set of simple paths $P(u, v)$ between vertices $u$ and $v$.\nA static application graph is modeled as weighted graph $A=(V_A, \\omega_A)$\nTopology mapping considers mapping $\\sigma : V_A \\mapsto V_H$. $\\sigma$ assigns each vertex $s \\in V_A$ in the application graph a target vertex $t \\in V_H$ in the architecture (host) graph.\nCost metrics\nDilation is maximum or sum of the pairwise distances of neighbors in $A$ mapped to $H$. Let $d_H(x,y)$ be shortest distance vertices $x, y\\in V_H$, the weighted sum of dilation is defined as $$\\text{Dilation}(\\sigma) = \\sum_{u,v \\in V_A} d_H(\\sigma(u), \\sigma(v)) \\times \\omega(u, v).$$\nCongestion counts how many communication pairs use a certain link. Let $p_e(u,v)$ be the probability that any of the routes from $u$ to $v$ crosses an edge $e \\in V_H$, congestion of this edge $e$ is $C_e=\\sum_{u,v\\in V_A} p_e(u,v)$. The maximum congestion $C_{max}=\\max_{e}C_e$ often correlates strongly with the execution time of Bulk Synchronous Parallel (BSP) application.\n$A=(V_A, \\omega)$ Application graph\n$\\omega(u,v)$ weight of the edge represents the volume of communication\n$H=(V_H, C_H, c_H, R_H)$ Host graph\n$C_H(u)$ is the number of processes that can be hosted at $u$, $C_H(u)=0$ for a switch\n$c_H(u,v)$ is the capacity (bandwidth) of link\n$R_H(u,v)$ routing algorithm\n$$\\text{Dilation}(u,v) = \\sum_{p\\in P(\\sigma(u),\\sigma(v))} R_H(\\sigma(u), \\sigma(v))(p) \\times |p|$$\n$$\\text{Dilation}(\\sigma) = \\sum_{u,v \\in A} \\omega(u,v) \\times \\text{Dilation}(u,v)$$\n$$\\text{Traffic}(e) = \\sum_{u,v \\in A} \\omega(u,v)(\\sum_{p} R_H(\\sigma(u), \\sigma(v))(p))$$\n$$\\text{Congestion}(e) = \\text{Traffic}(e) / c_H(e)$$\n$$\\text{Congestion}(\\sigma) = \\max_{e} \\text{Congestion}(e)$$\n"},{"id":3,"href":"/docs/neural-network/","title":"Neural Network","section":"Docs","content":" Multilayer Perceptrons # The model of each neuron in the network includes a nonlinear activation function that is differentiable. Let $T = {x(n), t(n)}_{n=1}^N$ denote the training sample. Let $y_i(n)$ denote the function signal produced by output neuron $j$. The error signal produced at neuron $j$ is defined by\n$$ \\begin{aligned} e_j(n) \u0026amp;= d_j(n) - y_j(n)\\\\ \u0026amp;= t_j(n) - y_j(n) \\end{aligned} $$\nThe instantaneous error energy of neuron $j$ is defined by $E_j(n) = e_j^2(n)/2$.\nTotal instantaneous error energy of the whole network $E(n) = \\sum_{j \\in C} E_j(n)$ where $C$ includes all neurons in output layer.\nWith $N$ training samples, the error energy averaged over the training sample or empirical risk is\n$$ \\begin{aligned} E_{av}(N) \u0026amp;= \\frac{1}{N} \\sum_{n=1}^{N} \\mathcal{E}(n)\\\\ \u0026amp;= \\frac{1}{2} \\sum_{j \\in C} e_j^2(n) \\end{aligned} $$\n$$y(\\mathbf{x, w}) = \\sigma(\\sum_{i=1}^{D}w_{ji}x_i + w_{j0})$$\n$w^k_{ij}$ : weight for node $j$ in layer $k$ for incoming node $i$\n$b^k_i$ : bias for node $i$ in layer $k$\n$a^k_i$ : product sum plus bias (activation) for node $i$ in layer $k$\n$o^k_i$ : output for node $i$ in layer $k$\n$r_k$ : number of nodes in layer $k$\nThe output layer $$\\delta^m_j = (\\hat{y}-y) f\u0026rsquo;(a^m_j)$$\nThe hidden layer $$\\delta^k_j = f\u0026rsquo;(a^k_j) \\sum_{l=1}^{r^{k+1}} w_{jl}^{k+1} \\delta_{l}^{k+1}$$\n"},{"id":4,"href":"/docs/transaction/","title":"Distributed Transactions","section":"Docs","content":" Occult (Observable causal consistency using lossy timestamps) # Observable causal consistency definition:\nAlice write(a=1) -\u0026gt; |Shard A| --x--\u0026gt; |Shard A\u0026#39;| Bob read(a)=1 \u0026lt;- |Shard A| write(b=2) -\u0026gt; |Shard B| -----\u0026gt; |Shard B\u0026#39;| -\u0026gt; read(b) waits for read(a) All existing causally consistent systems are susceptible to slowdown cascades.\nSolution: timestamp like vector clocks Write path: Client library attaches its causal timestamp to every write and sends it to the master of the corresponding shard. The master increments the relevant time stamp, updates the received causal timestamp, and stores it with new value. Then asynchronously replicates the writes to its slaves before returning to client library.\nRead path: Read from any slave shard with higher timestamp, or retry read from master. Update local timestamp accordingly.\nTransaction isolations:\nSnapshot Isolation\nParallel Snapshot Isolation (PSI)\nPer-Client Snapshot Isolation (PC-PSI) \u0026ndash; Occult\nRead phase: client $c$ reads from local server for every object $o$ in transaction $T$ and store it as successful reads if it\u0026rsquo;s not stale compare to local shard timestamp. Meanwhile, buffer all writes.\nValidation phase:\nCommit phase:\nConsus # Consus implements transactions with three phase commit protocol and writes the value into a Generalized Paxos system.\nAssuming data is fully replicated between data centers, the transaction initiator $dc_1$ executes transaction $t$ and broadcast $t$ with its result state $s$ to all other data centers. Other data centers then re-executes $t$ and compare their result state to $s$.\nThe commit protocol in consus is too optimistic, would result in high abort rate under heavy write load. There is no evaluation results in the paper.\nSLCP (Simple Leaderless Consistency Protocol) # Client Servers | | | | X--------\u0026gt;|-\u0026gt;|-\u0026gt;| Write Request (sequence, id) |\u0026lt;--------X--X | Response | | | | | | | | X--------\u0026gt;|-\u0026gt;|-\u0026gt;| Read Request (with retry) |\u0026lt;--------X--X | This protocol essentially implement an atomic storage system that provides strong consistency that observable to clients. But it compares the performance with Multi-Paxos and Raft, which are consensus protocols that solves a harder problem.\nThe SLCP protocol basically uses a majority write and read quorums. Compare to the majority replication algorithm, SLCP does not require two phases for write operation, and reads are not guaranteed to complement in certain number of rounds.\n"},{"id":5,"href":"/docs/wankeeper/","title":"WanKeeper: Efficient Distributed Coordination at WAN-scale","section":"Docs","content":"In recent years, there have been more systems to distribute data at global scale, especially so given the rise of the NewSQL systems like Google Spanner 1, CockroachDB and Microsoft Cosmos DB. While providing the same scalable performance of NoSQL systems, they still maintain strong consistency guarantees by relying upon some forms of wide area network (WAN) coordination service. Among them, ZooKeeper 2 is the most popular coordination service used in many projects in Facebook and Apache.\nZooKeeper implements the Zab 3 protocol that, similar to Multi-Paxos 4 and Raft 5, relies on the stable leader $\\lambda$ to serialize the total order of events. Every replica in the system only communicates with $\\lambda$ unless there\u0026rsquo;s leader failure. We call this type of systems centralized coordination. A centralized coordination cannot be used in WAN because its performance limitations, specifically, it fails to scale with respect to increasing distance from a replica to stable leader. Each operation will suffer from WAN latency.\nOther protocols, like Generalized Paxos 6 and EPaxos 7, opportunistically allow decentralized coordination by depending on a fast quorum to detect any conflicts before the value is committed. The fast quorum size is much larger than a simple quorum, therefore, it fails to scale with respect to increasing number of nodes in a globally deployed system.\nOne simple solution to scale ZooKeeper to multiple sites is running separate ZooKeeper cluster in each site, then maintain consistency between clusters using external method like 2 phase commit (2PC) or sync operations. Such solution has two major drawbacks: (1) data in each cluster is statically partitioned, therefore performance is limited by the operations targeting nonlocal data; and (2) any inter-site coordination (e.g. 2PC) is complicated and does not scale with more sites.\nWanKeeper provides a novel hybrid framework that extends centralized coordination by using hierarchical composition and token migration ideas. WanKeeper architecture is simple by design. We designate one cluster as the higher level (L2) cluster, preferably in the geographical center of the global system. L2-cluster\u0026rsquo;s task is to serialize conflicting requests from any cluster and serve tokens to the suitable cluster while ensuring freedom from deadlocks and starvation. A token for each object in the system represents the exclusive ownership of that object. If tokens for all the objects needed for an operation are present at the cluster, the operation is executed at that cluster without involving communication to L2 for coordination. If an operation involves items for which tokens are not present in this cluster, the leader forwards the operation-request to L2.\nWe argue that the hierarchical architecture is more scalable because it reduces the communication complexity and exploits the locality exhibited in the workload.\nCorbett, James C., et al. \u0026ldquo;Spanner: Google’s globally distributed database.\u0026rdquo; ACM Transactions on Computer Systems (TOCS) 31.3 (2013): 8.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHunt, Patrick, et al. \u0026ldquo;ZooKeeper: Wait-free Coordination for Internet-scale Systems.\u0026rdquo; USENIX annual technical conference. Vol. 8. 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJunqueira, Flavio P., Benjamin C. Reed, and Marco Serafini. \u0026ldquo;Zab: High-performance broadcast for primary-backup systems.\u0026rdquo; Dependable Systems \u0026amp; Networks (DSN), 2011 IEEE/IFIP 41st International Conference on. IEEE, 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nVan Renesse, Robbert, and Deniz Altinbuken. \u0026ldquo;Paxos made moderately complex.\u0026rdquo; ACM Computing Surveys (CSUR) 47.3 (2015): 42.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOngaro, Diego, and John K. Ousterhout. \u0026ldquo;In Search of an Understandable Consensus Algorithm.\u0026rdquo; USENIX Annual Technical Conference. 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLamport, Leslie. \u0026ldquo;Generalized consensus and Paxos.\u0026rdquo; Technical Report MSR-TR-2005-33, Microsoft Research, 2005.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMoraru, Iulian, David G. Andersen, and Michael Kaminsky. \u0026ldquo;There is more consensus in egalitarian parliaments.\u0026rdquo; Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles. ACM, 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":6,"href":"/docs/paxos_reconfiguration/","title":"Paxos Reconfiguration","section":"Docs","content":" Introduction # There have been several approaches to dynamic reconfiguration of consensus algorithms. Dynamic Paxos (Multi-Paxos that supports reconfig) limits the number of concurrent proposals as a pipeline length parameter $\\alpha \u0026gt; 0$, therefore a configuration change chosen at instance $i$ does not take effect until instance $i + \\alpha$. Raft protocol supports unbounded pipeline by limiting reconfiguration operations to be only add or remove one node at a time. Vertical Paxos and Egalitarian Paxos employ a separate oracle to manage the configuration 1. Zab added a limited-pipeline approach much like Dynamic Paxos 2.\nFlexible Paxos Reconfiguration # Flexible Paxos generalized Paxos quorums from\nAny two quorums intersect $$q_1, q_2 \\in Q \\implies q_1 \\cap q_2 \\neq \\emptyset$$\nto\nPhase I and Phase II quorums intersect $$q_1 \\in Q^{I}, q_2 \\in Q^{II} \\implies q_1 \\cap q_2 \\neq \\emptyset$$\nTurner 3 describes the unbounded reconfiguration algorithm for Flexible Paxos.\nMoraru, Iulian, David G. Andersen, and Michael Kaminsky. \u0026ldquo;There is more consensus in egalitarian parliaments.\u0026rdquo; Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles. ACM, 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJunqueira, Flavio P., Benjamin C. Reed, and Marco Serafini. \u0026ldquo;Zab: High-performance broadcast for primary-backup systems.\u0026rdquo; Dependable Systems \u0026amp; Networks (DSN), 2011 IEEE/IFIP 41st International Conference on. IEEE, 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTurner, David C. \u0026ldquo;Unbounded Pipelining in Dynamically Reconfigurable Paxos Clusters.\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":7,"href":"/docs/sonification/","title":"What Paxos sounds like","section":"Docs","content":" Sonification is the use of non-speech audio to convey information or perceptualize data. Auditory perception has advantages in temporal, spatial, amplitude, and frequency resolution that open possibilities as an alternative or complement to visualization techniques. \u0026ndash; Wikipedia\nFollowing simple rules, to assign different audio frequencies to meaningful events or state changes in a computer program, we were able to clearly hear the \u0026ldquo;characteristic\u0026rdquo; and pattern that distinguish them, like the sorting algorithms in this video.\nWhat about a distributed algorithm? When multiple processes joined to play the sound, will it become an orchestra? In this fun idea, we tried to make Paxos implementation from this paper perform.\nWith 5 Acceptors, 2 Replicas and 2 Leaders, each process will play a different MIDI instruments, and different event in the Paxos system is assigned with different note. To make it sounds better, we delay every event for 150 milliseconds, so that each note is played 150ms.\nYour browser does not support the audio element. Since there is no deterministic execution in any distributed systems (uncertain arrival of messages), the sound in realtime is different every time we run the program, but the pattern remains. Our experimental program can be downloaded here, which can be run in terminal in following commands:\njava -jar paxos-sonification.jar "},{"id":8,"href":"/docs/compressive_sensing/","title":"Compressive Sensing","section":"Docs","content":" Introduction # This survey is inspired by a seminar on the topic of crowd sensing systems. In which I have been exposed to the celebrated theory of compressive sensing, also known as compressive sampling or CS, a novel sensing paradigm that can help reduce the sampling rate of sensing tasks. Sampling is data acquisition protocol that aims to recover the original signal of interest. Such signals includes audio, medical images, radio, computer visions, etc. Traditionally, Shannon’s theorem states that: the sampling rate must be at least twice the maximum frequency present in the signal (Nyquist rate). However, CS suggest that it is possible to surpass the traditional limits, by the fact that we can represent many signals using only a few non-zero coefficients in a suitable basis or dictionary, utilizing nonlinear optimization to enable recovery of such signals from the small set of measurements. Another way to look at it is that rather than measuring at a high rate first, then compressing the sampled data, CS directly sense the data in a compressed form.\nIn this survey, we first provide a detailed review of the mathematical foundation underlying compressive sensing problem. Then we present the extensions of CS techniques.\nCompressive Sensing Primer # Signals # Signals can be treated as real-valued functions $f$, with continuous or discrete domains, finite or infinite. Signals produced by natural or man-made systems, usually has discrete, finite domain. We can model such signal as linear structure as vectors living in an $n$-dimensional Euclidean vector space, denoted by $\\mathbb{R}^n$. Vector space allow us to add two signals together to generate a new, physically meaningful signal. Other geometry tools such as lengths, distances, angles can also be used to describe and compare signals of interest.\nThe length of a vector $x = (x_1, x_2, \u0026hellip;, x_n)$ in $n$-dimensional real vector space $\\mathbb{R}^n$ can be given by the Euclidean norm: $$|x|_2 = (x_1^2 + x_2^2 + \u0026hellip; + x_n^2)^{\\frac{1}{2}}$$ The Euclidean distance between two points $x$ and $y$ is the length $|x-y|_2$ of the straight line between the two points. However, the Euclidean distance is insufficient in many situations of a given space. Instead, we make frequent use of the $p$-norms, which are defined for $p \\in [0,\\infty]$ as\n$$ |x|p = \\begin{cases} | \\text{supp}(x) | \u0026amp; \\quad p = 0; \\ ( \\sum{i=1}^{n} | x_i |^p )^{\\frac{1}{p}} \u0026amp; \\quad p \\in [1, \\infty); \\ \\text{max} | x_i | \u0026amp; \\quad p = \\infty \\ \\end{cases} $$\nwhere |supp($x$)| = { $i : x_i \\neq 0$ } denotes the cardinality of support of $x$. Also note that, in standard inner product in $\\mathbb{R}^n$, $$ \\langle x, x \\rangle = x^Tx = \\sum_{i=1}^{n}x_i x_i $$ thus $\\ell_2$ norm can be writen as $|x|_2 = \\sqrt{\\langle x,x \\rangle}$. The norms are typically used as a measure of the strength of a signal or size of an error.\nBases # Any vector $f \\in \\mathbb{R}^n$ can be represented by a linear combination of basis vectors ${ \\psi_i }{i=1}^{n}$ and (unique) coefficients ${x_i}{i=1}^{n}$, such that $$f(t) = \\sum_{i=1}^{n} x_i \\psi_i(t)$$ Let $\\Psi$ denote $n \\times n$ matrix with $\\psi_i$ as columns, $x$ denote vector with entries $x_i = \\langle f,\\psi_i \\rangle$, we can rewrite the equation more compactly as $$f = \\Psi x$$ Discrete Fourier base and discrete cosine base are examples of typical choices for the sparsifying base $\\Psi$.\nSparsity # A signal $f$ is $S$-sparse when it has at most $S$ nonzeros, i.e., $|f|0 \\leq S$. Signals that are not themselves sparse, but can expressed as $f=\\Psi x$ in some basis $\\Psi$, where $|x|0 \\leq S$, can still be refered as $S$-sparse. When the signal has a sparse expansion, we can discard the small coefficients without much perceptual loss. Formally, $f_S(t)$ is obtained by keeping only the terms corresponding to the $S$ largest values of $x_i$ in above equation, such that $f_S := \\Psi x_S$. We have $$ | f - f_S |{\\ell_2} = | x - x_S |{\\ell_2} $$ and if the sorted magnitudes of $x_i$ decay quickly, we say $x$ is sparse or compressible, then $x$ is well approximated by $x_S$, therefore, the error $| f-f_S |_{\\ell_2}$ is small. In other words, a large fraction of the coefficients can be ignored (set to 0) without much loss.\nSensing Matrices # In this survey, we only consider the standard finite dimensional CS model. Specifically, given a signal $f \\in \\mathbb{R}^n$, we consider measurement systems that acquire $m$ linear measurements. This process is mathematically represent as $$ y_k = \\langle f, \\phi_k \\rangle $$ where $k = 1,\u0026hellip;,m$. Let $A$ denote the $m\\times n$ sensing matrix with vectors $ \\phi_1^* ,\u0026hellip;, \\phi_m^* $ as rows (* is the complex transpose), we can rewrite equation as $$ y = Af $$ The matrix $A$ represents a dimensionality reduction, i.e., it maps $\\mathbb{R}^n$ into $\\mathbb{R}^m$ where $n\\gg m$. We assume that $f$ is a finite-length vector with a discrete-valued index (such as time or space).\nIncoherent Sampling # With given pair $(\\Phi, \\Psi)$ of orthobases of $\\mathbb{R}^n$, $\\Phi$ is sampling matrix as in equation~\\ref{eq:measure}, $\\Psi$ is basis used to represent $f$ in equation~\\ref{eq:basis}. Definition 1 The coherence between the sensing basis $\\Phi$ and representation basis $\\Psi$ is \\begin{equation} \\mu(\\Phi, \\Psi) = \\sqrt{n} \\cdot \\max_{1\\leq k,j\\leq n} |\\langle \\phi_k, \\psi_j \\rangle|. \\end{equation} The coherence measures the largest correlation between any two elements of $\\Phi$ and $\\Psi$ \\cite{uncertainty}. If there is correlated elements, the coherence is large. Compressive sampling is mainly concerned with low coherence pairs.\nRandom matrices are highly incoherent with any fixed basis $\\Psi$. If we select $\\Phi$ uniformly at random, then with high probability, the coherence between $\\Psi$ and $\\Phi$ is $\\mu(\\Phi, \\Psi) = \\sqrt{2\\log n}$.\nSignal Recovery via $\\ell_1$ Minimization # Key Notations $n$ Size of original signal $m$ Number of measurements $S$ Size of the support of the signal in sparse domain $f$ Original signal $x$ Coefficients $e$ Error in sample data $\\Phi$ $m \\times n$ sampling matrix $\\Psi$ $m \\times n$ sparsifying matrix $A=\\Phi\\Psi$ Sensing matrix To summarize above sections, consider $f\\in\\mathbb{R}^n$ is a target signal we would like to reconstruct, $f$ can be decomposed under a certain base $\\Psi$, i.e. $f = \\Psi x$ where $x$ is the coefficient vector. $\\Phi$ is a linear encoder which projects an $n$-dimensional data into an $m$-dimensional subspace ($m \u0026lt; n$). CS allows us to reconstruct sparse $f$ from its linear measurements \\begin{equation} s = \\Phi f = \\Phi \\Psi x \\label{eq:recover} \\end{equation} In above equation, $m$ can be as small as $O(S\\log\\frac{n}{S})$ \\cite{compressive}. The Sensing matrix $A$ as mentioned in Table 1 has to satisfy the Restricted Isometry Property (RIP) \\cite{csstable}.\nFinally, we can perform the $\\ell_1$-norm minimization \\begin{equation} (\\mathrm{P}1) \\text{arg min}{\\hat{x} \\in \\mathbb{R}^n} |\\hat{x} |{\\ell_1}, \\text{ subject to } \\Phi\\Psi\\hat{x} = y = Ax \\label{eq:l1min} \\end{equation} where $|x|{\\ell_1} := \\sum_i |x_i|$.\nThis $\\ell_1$-norm minimization replaces the NP-hard $\\ell_0$-norm minimization that directly searches for the sparsest $\\hat{x}$. $\\ell_1$-min can be solved in polynomial time by linear programming, besides, various greedy algorithms are also practical alternatives.\nExtensions # Robust Compressive Sampling # In section~\\ref{sec:math} we have show that how to recover sparse signals from a few measurements. However, to be powerful in practice, CS needs to be able to deal with nearly sparse signals and with noise. In real applications, measured data will inevitably be corrupted by some noise as the sensing device does not have infinite precision. Here, our goal is that small perturbations in the data should cause small perturbations in the reconstruction. Recovering a vector $x\\in\\mathbb{R}^n$ from data $$ y = Ax + e$$ where $A$ is the same $m\\times n$ sensing matrix, and $e$ is a stochastic or deterministic unknown error term. We can modify the $\\ell_1$ minimization with relaxed constraints for reconstruction: \\begin{equation} (\\mathrm{P}1) \\text{arg min}{\\hat{x} \\in \\mathbb{R}^n} |\\hat{x} |{\\ell_1}, \\text{ subject to } |\\Phi\\Psi\\hat{x} - y |{\\ell_2} \\leq e \\label{eq:l1min2} \\end{equation}\nHybrid CS # In some problems, the compressive sensing model can be deployed in a hybrid way, in particular, CS is not to carry all the load. In such deployment, we can apply CS to measure only fine scale properties of the signal, while ordinary linear measurement and reconstruction was used to obtain the coarse scale properties of the signal. For example, we can expand the object $x_0$ in the wavelet basis $$x_0 = \\sum_k \\beta_{j_0,k}\\phi_{j_0,k} + \\sum_{j=j_0}^{j_1}\\sum_k \\alpha_{j,k}\\psi_{j,k}$$ where $j_0$ is some specified coarse scale, $j_1$ is the finest scale, $\\phi_{j_0,k}$ are mail wavelets at coarse scale and $\\psi_{j,k}$ are fine scale female wavelets. $\\alpha$ denote the grouping together of all wavelet coefficients, and $\\beta$ denote the male coefficients. For male coarse scale coefficients, we take direct measurements, whereas for the female fine scale coefficients, we apply the CS scheme. Let $$ y = \\Phi\\Psi^T x_0 $$ To reconstruct from these observations, consider the basis-pursuit optimization problem \\begin{equation} (\\mathrm{BP}) \\min_{\\hat{x}} |\\hat{x}|_{\\ell_1} \\text{ subject to } y_n = \\Phi\\Psi^T\\Psi \\hat{x} \\end{equation} Results show that Hybrid CS reconstruct signals with even fewer samples and the accuracy is evidently comparable.\nMultiscale CS # Inspired by the success of Hybrid CS, we may consider a fully multiscale deployment of CS. We may expand the object $x_0$ in the wavelet basis in the same way, then partition the coefficient vector as $[\\beta_{j_0,\\cdot}, \\alpha_{j_0,\\cdot},\u0026hellip;,\\alpha_{j_1-1,\\cdot}]$. We then apply ordinary linear samping to measure the coefficients $\\beta_{j_0,\\cdot}$ directly, and then separately apply compressed sensing scale-by-scale, sampling data $y_j$ about the coefficients $\\alpha_{j,\\cdot}$ at level $j$ using an $n_j \\times 2^j$ CS matrix $\\Phi_j$. To obtain a reconstruction, we solve the sequence of problems \\begin{equation} (\\mathrm{BP_j}) \\min_{\\hat{x}} |\\hat{x}|_{\\ell_1} \\text{ subject to } y_j = \\Phi_j \\hat{x} \\end{equation}\nCost Aware CS # In applications like mobile crowd-sensing or wireless sensor networks, the resource burden of collecting samples is usually a major concern. However, the ordinary CS assumes that every sample has the same cost, it simply reduce the number of samples while the Cost Aware CS takes the cost of each sample into consideration. The new objective becomes not only maximizing recovery accuracy, also minimizing the sampling cost. CACS in \\cite{cacs} uses Regularized Column Sum (RCS) to predict recovery accuracy, which is equivalent to Statistical RIP (Restricted Isometry Property) condition, to formulate the RCS-constrained Optimization (RO) to fund an optimal randomized sampling strategy $\\pi$. In order to make RO solvable, the paper uses two relaxation methods: \\begin{align} \\sigma(F_{\\Omega}) \\leq \\alpha \\quad \u0026amp; \\to \\quad \\mathbb{E}[\\sigma(F_{\\Omega})] \\leq \\alpha \\ \\pi \\in {0,1}^n \\quad \u0026amp; \\to \\quad \\pi \\in [0,1]^n \\end{align}\nThen we can use the following convex optimization problem to find an optimal randomized sampling strategy $\\pi$ which satisfies the given RCS constraint with the lowest cost. \\begin{align*} (\\mathrm{P}) \\min_{\\pi} \\quad \u0026amp; c^T \\pi \\ \\text{subject to} \\quad \u0026amp; 1^T \\pi = m \\ \\quad \u0026amp; (Re(F_j)^T \\pi)^2 + (Im(F_j)^T \\pi)^2 \\leq \\alpha^2 \\ \\quad \u0026amp; 0 \\leq \\pi_i \\leq 1, i = 1,\u0026hellip;,n. \\end{align*} where $c$ is the cost map, $\\pi$ is the sampling strategy, $m$ is the expected sample size, and $Re(F_j)$ and $Im(F_j)$ denote the real and imaginary component of the $j$th column in $F$. Above problem can be solved in polynomial time via standard interior point methods.\nHowever, RO is centralized algorithm since it requires global information of the cost map, which makes it impractical. The paper further provides two decentralized algorithm that only uses partial information.\nDistributed CS # Distributed Compressive Sensing (DCS) \\cite{dcs} improves the signal recovery performance of multi signal ensembles by exploiting both intra- and inter-signal correlation and sparsity structure. DCS is particularly useful in a scenario of multiple sensors carry out the compression in a distributed way without cooperation with each other and transmit the compressed signal to the sink node. At the sink node, the received signals from all the sensors are recovered jointly. The key of DCS is joint sparsity, defined as the sparsity of the entire signal ensemble. DCS consider three types of models as joint sparse signal:\nEach signal is individually sparse, there are also common components shared by every signal, namely common information. This allows joint recovery with a reduced measurements. All signals share the supports, the locations of the nonzero coefficients. No signal is sparse itself, nevertheless, they share the large amount of common information.\nOne example application for DCS is in \\cite{multimodal}, while combining multiple images of the same scene into a single image, they made the constant background image as common information and the variable foreground image as innovation information for efficiency of the process.\nGeneralized Distributed CS # The Generalized Distributed Compressive Sensing (GDCS) \\cite{gdcs} can improve sparse signal detection performance given arbitrary types of common information which are classified into not just full common information as DCS, but also a variety of partial common information.\n"},{"id":9,"href":"/docs/tla+/","title":"TLA+ wiki","section":"Docs","content":" Modules # Integers Naturals Reals Sequences TLC \u0026ndash; print Bags are multi-set functions, each element points to an integer as its quntity. Defines operators: (+) (-) FiniteSets defines IsFiniteSet(S) and Cardinality(S). RealTime \u0026ndash; RTBound RTnow now Definition # \u0026lt;A\u0026gt;_v == A /\\ (v\u0026#39; # v) $$\\langle A \\rangle_v == A \\land (v\u0026rsquo; \\neq v)$$\n[A]_v == A \\/ (v\u0026#39; = v) $$[A]_v == A \\lor (v\u0026rsquo;=v)$$\nOperators # Divides (p, n) == $\\exists q \\in Int : n = q * p$\nDivisorsOf (n) == {$p \\in Int$ : Divides(p, n)}\nSetMax (S) == CHOOSE $i \\in S$ : $\\forall j \\in S$ : $i \\geq j$\nGCD (m, n) == SetMax(DivisorsOf(m) $\\cap$ DivisorsOf(n))\nSetGCD(T) == SetMax( {$d \\in Int$ : $\\forall t \\in T$ : Divides(d, t)} )\nmax(x, y) == IF x\u0026gt;y THEN x ELSE y\nmin(x, y) == IF x\u0026lt;y THEN x ELSE y\nmaxv(x, y) == [$i \\in$ DOMAIN x |-\u0026gt; max(x[i], y[i])]\nCardinality(set) SortSeq(s, \u0026lt;) Len(s) Head(s) Tail(s) Append(s,e) Seq(s) SubSeq(s, m, n) SelectSeq(s, op) successor[$i \\in Nat$] == i+1\nsuccessor == [$i \\in Nat$ |-\u0026gt; i+1]\nfactorial[$n \\in Nat$] == IF n = 0 THEN 1 ELSE n * factorial[n-1]\nRECURSIVE FactorialOp(_)\nFactorialOp(n) == IF n=0 THEN 1 ELSE n * Factorial(n-1)\nRECURSIVE Cardinality(_)\nCardinality(S) == IF S={} THEN 0 ELSE 1+Cardinality(S \\ {CHOOSE $x \\in S$ : TRUE})\nSortings(S) == LET D == 1..Cardinality(S)\nIN {$seq \\in [D \\to S] : $\n$\\land S \\subseteq {seq[i] : i \\in D}$\n$\\land \\forall i,j \\in D : (i\u0026lt;j) \\Rightarrow (seq[i].key \\leq seq[j].key)$}\nRECURSIVE SetSum(_)\nSetSum(S) == IF S={} THEN 0 ELSE LET s==CHOOSE $x \\in S$ : TRUE IN s + SetSum(S \\ {s})\nRECURSIVE SeqSum(_)\nSeqSum(s) == IF s=\u0026laquo;\u0026raquo; THEN 0 ELSE Head(s) + SeqSum(Tail(s))\nNetwork # variable network = [from $\\in 1..N$ |-\u0026gt; [to $\\in 1..N$ |-\u0026gt; \u0026laquo;\u0026raquo;]];\ndefine { send(from, to, msg) == [network EXCEPT ![from][to] = Append(@, msg)] bcast(from, msg) == [network EXCEPT ![from] = [to $\\in 1..N$ |-\u0026gt; Append(network[from][to], msg)]] } macro rcv() { with (from $\\in$ {$j \\in 1..N$ : Len(network[j][self]) \u0026gt; 0}) { msg := Head(network[from][self]); network[from][self] := Tail(@) }; } "}]