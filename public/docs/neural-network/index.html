<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Multilayer Perceptrons # The model of each neuron in the network includes a nonlinear activation function that is differentiable. Let $T = {x(n), t(n)}_{n=1}^N$ denote the training sample. Let $y_i(n)$ denote the function signal produced by output neuron $j$. The error signal produced at neuron $j$ is defined by
$$ \begin{aligned} e_j(n) &= d_j(n) - y_j(n)\\ &= t_j(n) - y_j(n) \end{aligned} $$
The instantaneous error energy of neuron $j$ is defined by $\mathcal{E}_j(n) = e_j^2(n)/2$."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content="Neural Network"><meta property="og:description" content="Multilayer Perceptrons # The model of each neuron in the network includes a nonlinear activation function that is differentiable. Let $T = {x(n), t(n)}_{n=1}^N$ denote the training sample. Let $y_i(n)$ denote the function signal produced by output neuron $j$. The error signal produced at neuron $j$ is defined by
$$ \begin{aligned} e_j(n) &= d_j(n) - y_j(n)\\ &= t_j(n) - y_j(n) \end{aligned} $$
The instantaneous error energy of neuron $j$ is defined by $\mathcal{E}_j(n) = e_j^2(n)/2$."><meta property="og:type" content="article"><meta property="og:url" content="/docs/neural-network/"><meta property="article:section" content="docs"><meta property="article:published_time" content="2021-05-29T00:00:00+00:00"><meta property="article:modified_time" content="2021-05-29T00:00:00+00:00"><title>Neural Network | Principia</title><link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.a82d7e77ceb134d151c4d7e381eeb30623fbd5a524d58c584d8716ecec0205bd.css><script defer src=/flexsearch.min.js></script>
<script defer src=/en.search.min.fcc7afe81cf809df1c8e231bd2844394a505ccd64887600277b6d503389ba15f.js></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>Principia</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><span>Docs</span><ul><li><a href=/docs/abd/>ABD Algorithm</a></li><li><a href=/docs/model-checker/>Model Checker in Go</a></li><li><a href=/docs/topology-mapping/>Topology Mapping Algorithm</a></li><li><a href=/docs/neural-network/ class=active>Neural Network</a></li><li><a href=/docs/transaction/>Distributed Transactions</a></li><li><a href=/docs/wankeeper/>WanKeeper: Efficient Distributed Coordination at WAN-scale</a></li><li><a href=/docs/paxos_reconfiguration/>Paxos Reconfiguration</a></li><li><a href=/docs/sonification/>What Paxos sounds like</a></li><li><a href=/docs/tla+/>TLA+ wiki</a></li></ul></li></ul><ul><li><a href=https://github.com/ailidani target=_blank rel=noopener>Github</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Neural Network</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><a href=#multilayer-perceptrons>Multilayer Perceptrons</a></li></ul></li></ul></nav></aside></header><article class=markdown><h2 id=multilayer-perceptrons>Multilayer Perceptrons
<a class=anchor href=#multilayer-perceptrons>#</a></h2><p>The model of each neuron in the network includes a nonlinear activation function that is differentiable.
Let $T = {x(n), t(n)}_{n=1}^N$ denote the training sample.
Let $y_i(n)$ denote the function signal produced by output neuron $j$.
The error signal produced at neuron $j$ is defined by</p><p>$$
\begin{aligned}
e_j(n) &= d_j(n) - y_j(n)\\
&= t_j(n) - y_j(n)
\end{aligned}
$$</p><p>The instantaneous error energy of neuron $j$ is defined by $\mathcal{E}_j(n) = e_j^2(n)/2$.</p><p>Total instantaneous error energy of the whole network $\mathcal{E}(n) = \sum_{j \in C} \mathcal{E}_j(n)$ where $C$ includes all neurons in output layer.</p><p>With $N$ training samples, the error energy averaged over the training sample or empirical risk is</p><p>$$
\begin{aligned}
\mathcal{E}<em>{av}(N) &= \frac{1}{N} \sum</em>{n=1}^{N} \mathcal{E}(n)\\
&= \frac{1}{2} \sum_{j \in C} e_j^2(n)
\end{aligned}
$$</p><p>$$y(\mathbf{x, w}) = \sigma(\sum_{i=1}^{D}w_{ji}x_i + w_{j0})$$</p><p>$w^k_{ij}$ : weight for node $j$ in layer $k$ for incoming node $i$</p><p>$b^k_i$ : bias for node $i$ in layer $k$</p><p>$a^k_i$ : product sum plus bias (activation) for node $i$ in layer $k$</p><p>$o^k_i$ : output for node $i$ in layer $k$</p><p>$r_k$ : number of nodes in layer $k$</p><p><em>The output layer</em>
$$\delta^m_j = (\hat{y}-y) f&rsquo;(a^m_j)$$</p><p><em>The hidden layer</em>
$$\delta^k_j = f&rsquo;(a^k_j) \sum_{l=1}^{r^{k+1}} w_{jl}^{k+1} \delta_{l}^{k+1}$$</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#multilayer-perceptrons>Multilayer Perceptrons</a></li></ul></li></ul></nav></div></aside></main><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script>
<script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script></body></html>