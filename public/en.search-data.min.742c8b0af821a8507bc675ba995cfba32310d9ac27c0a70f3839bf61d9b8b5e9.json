[{"id":0,"href":"/docs/neural-network/","title":"Neural Network","section":"Docs","content":"$$(\\mathbf{x, w}) = \\sigma(\\sum_{i=1}^{D}w_{ji}x_i + w_{j0})$$\n$w^k_{ij}$ : weight for node $j$ in layer $k$ for incoming node $i$\n$b^k_i$ : bias for node $i$ in layer $k$\n$a^k_i$ : product sum plus bias (activation) for node $i$ in layer $k$\n$o^k_i$ : output for node $i$ in layer $k$\n$r_k$ : number of nodes in layer $k$\nThe output layer\n$$\\delta^m_j = (\\hat{y}-y) f\u0026rsquo;(a^m_j)$$\nThe hidden layer\n$$\\delta^k_j = f\u0026rsquo;(a^k_j) \\sum_{l=1}^{r^{k+1}} w_{jl}^{k+1} \\delta_{l}^{k+1}$$\n"},{"id":1,"href":"/docs/transaction/","title":"Distributed Transactions","section":"Docs","content":" Occult (Observable causal consistency using lossy timestamps) # Observable causal consistency definition:\nAlice write(a=1) -\u0026gt; |Shard A| --x--\u0026gt; |Shard A\u0026#39;| Bob read(a)=1 \u0026lt;- |Shard A| write(b=2) -\u0026gt; |Shard B| -----\u0026gt; |Shard B\u0026#39;| -\u0026gt; read(b) waits for read(a) All existing causally consistent systems are susceptible to slowdown cascades.\nSolution: timestamp like vector clocks Write path: Client library attaches its causal timestamp to every write and sends it to the master of the corresponding shard. The master increments the relevant time stamp, updates the received causal timestamp, and stores it with new value. Then asynchronously replicates the writes to its slaves before returning to client library.\nRead path: Read from any slave shard with higher timestamp, or retry read from master. Update local timestamp accordingly.\nTransaction isolations:\nSnapshot Isolation\nParallel Snapshot Isolation (PSI)\nPer-Client Snapshot Isolation (PC-PSI) \u0026ndash; Occult\nRead phase: client $c$ reads from local server for every object $o$ in transaction $T$ and store it as successful reads if it\u0026rsquo;s not stale compare to local shard timestamp. Meanwhile, buffer all writes.\nValidation phase:\nCommit phase:\nConsus # Consus implements transactions with three phase commit protocol and writes the value into a Generalized Paxos system.\nAssuming data is fully replicated between data centers, the transaction initiator $dc_1$ executes transaction $t$ and broadcast $t$ with its result state $s$ to all other data centers. Other data centers then re-executes $t$ and compare their result state to $s$.\nThe commit protocol in consus is too optimistic, would result in high abort rate under heavy write load. There is no evaluation results in the paper.\nSLCP (Simple Leaderless Consistency Protocol) # Client Servers | | | | X--------\u0026gt;|-\u0026gt;|-\u0026gt;| Write Request (sequence, id) |\u0026lt;--------X--X | Response | | | | | | | | X--------\u0026gt;|-\u0026gt;|-\u0026gt;| Read Request (with retry) |\u0026lt;--------X--X | This protocol essentially implement an atomic storage system that provides strong consistency that observable to clients. But it compares the performance with Multi-Paxos and Raft, which are consensus protocols that solves a harder problem.\nThe SLCP protocol basically uses a majority write and read quorums. Compare to the majority replication algorithm, SLCP does not require two phases for write operation, and reads are not guaranteed to complement in certain number of rounds.\n"},{"id":2,"href":"/docs/wankeeper/","title":"WanKeeper: Efficient Distributed Coordination at WAN-scale","section":"Docs","content":"In recent years, there have been more systems to distribute data at global scale, especially so given the rise of the NewSQL systems like Google Spanner 1, CockroachDB and Microsoft Cosmos DB. While providing the same scalable performance of NoSQL systems, they still maintain strong consistency guarantees by relying upon some forms of wide area network (WAN) coordination service. Among them, ZooKeeper 2 is the most popular coordination service used in many projects in Facebook and Apache.\nZooKeeper implements the Zab 3 protocol that, similar to Multi-Paxos 4 and Raft 5, relies on the stable leader $\\lambda$ to serialize the total order of events. Every replica in the system only communicates with $\\lambda$ unless there\u0026rsquo;s leader failure. We call this type of systems centralized coordination. A centralized coordination cannot be used in WAN because its performance limitations, specifically, it fails to scale with respect to increasing distance from a replica to stable leader. Each operation will suffer from WAN latency.\nOther protocols, like Generalized Paxos 6 and EPaxos 7, opportunistically allow decentralized coordination by depending on a fast quorum to detect any conflicts before the value is committed. The fast quorum size is much larger than a simple quorum, therefore, it fails to scale with respect to increasing number of nodes in a globally deployed system.\nOne simple solution to scale ZooKeeper to multiple sites is running separate ZooKeeper cluster in each site, then maintain consistency between clusters using external method like 2 phase commit (2PC) or sync operations. Such solution has two major drawbacks: (1) data in each cluster is statically partitioned, therefore performance is limited by the operations targeting nonlocal data; and (2) any inter-site coordination (e.g. 2PC) is complicated and does not scale with more sites.\nWanKeeper provides a novel hybrid framework that extends centralized coordination by using hierarchical composition and token migration ideas. WanKeeper architecture is simple by design. We designate one cluster as the higher level (L2) cluster, preferably in the geographical center of the global system. L2-cluster\u0026rsquo;s task is to serialize conflicting requests from any cluster and serve tokens to the suitable cluster while ensuring freedom from deadlocks and starvation. A token for each object in the system represents the exclusive ownership of that object. If tokens for all the objects needed for an operation are present at the cluster, the operation is executed at that cluster without involving communication to L2 for coordination. If an operation involves items for which tokens are not present in this cluster, the leader forwards the operation-request to L2.\nWe argue that the hierarchical architecture is more scalable because it reduces the communication complexity and exploits the locality exhibited in the workload.\nCorbett, James C., et al. \u0026ldquo;Spanner: Googleâ€™s globally distributed database.\u0026rdquo; ACM Transactions on Computer Systems (TOCS) 31.3 (2013): 8.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHunt, Patrick, et al. \u0026ldquo;ZooKeeper: Wait-free Coordination for Internet-scale Systems.\u0026rdquo; USENIX annual technical conference. Vol. 8. 2010.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJunqueira, Flavio P., Benjamin C. Reed, and Marco Serafini. \u0026ldquo;Zab: High-performance broadcast for primary-backup systems.\u0026rdquo; Dependable Systems \u0026amp; Networks (DSN), 2011 IEEE/IFIP 41st International Conference on. IEEE, 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nVan Renesse, Robbert, and Deniz Altinbuken. \u0026ldquo;Paxos made moderately complex.\u0026rdquo; ACM Computing Surveys (CSUR) 47.3 (2015): 42.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOngaro, Diego, and John K. Ousterhout. \u0026ldquo;In Search of an Understandable Consensus Algorithm.\u0026rdquo; USENIX Annual Technical Conference. 2014.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLamport, Leslie. \u0026ldquo;Generalized consensus and Paxos.\u0026rdquo; Technical Report MSR-TR-2005-33, Microsoft Research, 2005.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMoraru, Iulian, David G. Andersen, and Michael Kaminsky. \u0026ldquo;There is more consensus in egalitarian parliaments.\u0026rdquo; Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles. ACM, 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":3,"href":"/docs/paxos_reconfiguration/","title":"Paxos Reconfiguration","section":"Docs","content":" Introduction # There have been several approaches to dynamic reconfiguration of consensus algorithms. Dynamic Paxos (Multi-Paxos that supports reconfig) limits the number of concurrent proposals as a pipeline length parameter $\\alpha \u0026gt; 0$, therefore a configuration change chosen at instance $i$ does not take effect until instance $i + \\alpha$. Raft protocol supports unbounded pipeline by limiting reconfiguration operations to be only add or remove one node at a time. Vertical Paxos and Egalitarian Paxos employ a separate oracle to manage the configuration 1. Zab added a limited-pipeline approach much like Dynamic Paxos 2.\nFlexible Paxos Reconfiguration # Flexible Paxos generalized Paxos quorums from\nAny two quorums intersect $$q_1, q_2 \\in Q \\implies q_1 \\cap q_2 \\neq \\emptyset$$\nto\nPhase I and Phase II quorums intersect $$q_1 \\in Q^{I}, q_2 \\in Q^{II} \\implies q_1 \\cap q_2 \\neq \\emptyset$$\nTurner 3 describes the unbounded reconfiguration algorithm for Flexible Paxos.\nMoraru, Iulian, David G. Andersen, and Michael Kaminsky. \u0026ldquo;There is more consensus in egalitarian parliaments.\u0026rdquo; Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles. ACM, 2013.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nJunqueira, Flavio P., Benjamin C. Reed, and Marco Serafini. \u0026ldquo;Zab: High-performance broadcast for primary-backup systems.\u0026rdquo; Dependable Systems \u0026amp; Networks (DSN), 2011 IEEE/IFIP 41st International Conference on. IEEE, 2011.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTurner, David C. \u0026ldquo;Unbounded Pipelining in Dynamically Reconfigurable Paxos Clusters.\u0026rdquo;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":4,"href":"/docs/sonification/","title":"What Paxos sounds like","section":"Docs","content":" Sonification is the use of non-speech audio to convey information or perceptualize data. Auditory perception has advantages in temporal, spatial, amplitude, and frequency resolution that open possibilities as an alternative or complement to visualization techniques. \u0026ndash; Wikipedia\nFollowing simple rules, to assign different audio frequencies to meaningful events or state changes in a computer program, we were able to clearly hear the \u0026ldquo;characteristic\u0026rdquo; and pattern that distinguish them, like the sorting algorithms in this video.\nWhat about a distributed algorithm? When multiple processes joined to play the sound, will it become an orchestra? In this fun idea, we tried to make Paxos implementation from this paper perform.\nWith 5 Acceptors, 2 Replicas and 2 Leaders, each process will play a different MIDI instruments, and different event in the Paxos system is assigned with different note. To make it sounds better, we delay every event for 150 milliseconds, so that each note is played 150ms.\nYour browser does not support the audio element. Since there is no deterministic execution in any distributed systems (uncertain arrival of messages), the sound in realtime is different every time we run the program, but the pattern remains. Our experimental program can be downloaded here, which can be run in terminal in following commands:\njava -jar paxos-sonification.jar "},{"id":5,"href":"/docs/tla+/","title":"TLA+ wiki","section":"Docs","content":" Modules # Integers Naturals Reals Sequences TLC \u0026ndash; print Bags are multi-set functions, each element points to an integer as its quntity. Defines operators: (+) (-) FiniteSets defines IsFiniteSet(S) and Cardinality(S). RealTime \u0026ndash; RTBound RTnow now Definition # \u0026lt;A\u0026gt;_v == A /\\ (v\u0026#39; # v) $$\\langle A \\rangle_v == A \\land (v\u0026rsquo; \\neq v)$$\n[A]_v == A \\/ (v\u0026#39; = v) $$[A]_v == A \\lor (v\u0026rsquo;=v)$$\nOperators # Divides (p, n) == $\\exists q \\in Int : n = q * p$\nDivisorsOf (n) == {$p \\in Int$ : Divides(p, n)}\nSetMax (S) == CHOOSE $i \\in S$ : $\\forall j \\in S$ : $i \\geq j$\nGCD (m, n) == SetMax(DivisorsOf(m) $\\cap$ DivisorsOf(n))\nSetGCD(T) == SetMax( {$d \\in Int$ : $\\forall t \\in T$ : Divides(d, t)} )\nmax(x, y) == IF x\u0026gt;y THEN x ELSE y\nmin(x, y) == IF x\u0026lt;y THEN x ELSE y\nmaxv(x, y) == [$i \\in$ DOMAIN x |-\u0026gt; max(x[i], y[i])]\nCardinality(set) SortSeq(s, \u0026lt;) Len(s) Head(s) Tail(s) Append(s,e) Seq(s) SubSeq(s, m, n) SelectSeq(s, op) successor[$i \\in Nat$] == i+1\nsuccessor == [$i \\in Nat$ |-\u0026gt; i+1]\nfactorial[$n \\in Nat$] == IF n = 0 THEN 1 ELSE n * factorial[n-1]\nRECURSIVE FactorialOp(_)\nFactorialOp(n) == IF n=0 THEN 1 ELSE n * Factorial(n-1)\nRECURSIVE Cardinality(_)\nCardinality(S) == IF S={} THEN 0 ELSE 1+Cardinality(S \\ {CHOOSE $x \\in S$ : TRUE})\nSortings(S) == LET D == 1..Cardinality(S)\nIN {$seq \\in [D \\to S] : $\n$\\land S \\subseteq {seq[i] : i \\in D}$\n$\\land \\forall i,j \\in D : (i\u0026lt;j) \\Rightarrow (seq[i].key \\leq seq[j].key)$}\nRECURSIVE SetSum(_)\nSetSum(S) == IF S={} THEN 0 ELSE LET s==CHOOSE $x \\in S$ : TRUE IN s + SetSum(S \\ {s})\nRECURSIVE SeqSum(_)\nSeqSum(s) == IF s=\u0026laquo;\u0026raquo; THEN 0 ELSE Head(s) + SeqSum(Tail(s))\nNetwork # variable network = [from $\\in 1..N$ |-\u0026gt; [to $\\in 1..N$ |-\u0026gt; \u0026laquo;\u0026raquo;]];\ndefine { send(from, to, msg) == [network EXCEPT ![from][to] = Append(@, msg)] bcast(from, msg) == [network EXCEPT ![from] = [to $\\in 1..N$ |-\u0026gt; Append(network[from][to], msg)]] } macro rcv() { with (from $\\in$ {$j \\in 1..N$ : Len(network[j][self]) \u0026gt; 0}) { msg := Head(network[from][self]); network[from][self] := Tail(@) }; } "}]